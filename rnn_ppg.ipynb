{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-ppg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwaGYlI9UMvA"
      },
      "source": [
        "**First block of code with imports and function definitions, to be executed only once:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqpfwfCQlI-C"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Softmax, Dropout, Bidirectional\n",
        "import scipy.io  # to load MAT files\n",
        "import time\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "import scipy.signal  # to downsample input data\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "print('Tf Keras:', keras.__version__)\n",
        "print('TensorFlow:', tf.__version__)\n",
        "print('GPU device:', tf.test.gpu_device_name())\n",
        "\n",
        "if 'google.colab' in sys.modules:  # try to detect if we're running in colab or locally\n",
        "  working_dir = '/content/drive/MyDrive/Colab Notebooks'\n",
        "else:\n",
        "  working_dir = '.'\n",
        "\n",
        "log_dir_base = working_dir + '/logs/fit'\n",
        "\n",
        "\n",
        "def create_dataset(window, overlap, decimation_factor = 0, distrib_stat = 0):\n",
        "  # Create the input and target data from PPG_ACC_dataset,\n",
        "  # according to window and overlap\n",
        "  dataset_dir = working_dir + '/PPG_ACC_dataset'\n",
        "  subj_list = [1, 2, 3, 4, 5, 6, 7]  # 1-based\n",
        "  x_data = np.empty((0, window, 4))\n",
        "  y_data = np.empty((0, 1))  # labels\n",
        "  subj_inputs = []  # number of inputs for every subject\n",
        "  print('### creating dataset')\n",
        "  # load data from PPG_ACC_dataset and reshape for RNN\n",
        "  tot_rows = 0\n",
        "  for subject in subj_list:\n",
        "    subj_inputs.append(0)\n",
        "    for category, name in enumerate(('rest', 'squat', 'step')):\n",
        "      for record in range(0, 5):\n",
        "        acc = scipy.io.loadmat(f'{dataset_dir}/S{subject}/{name}{record + 1}_acc.mat')['ACC']\n",
        "        ppg = scipy.io.loadmat(f'{dataset_dir}/S{subject}/{name}{record + 1}_ppg.mat')['PPG'][:, 0:2]  # some PPG files have 3 columns instead of 2\n",
        "        fusion = np.hstack((acc[:, 1:], ppg[:, 1:]))  # remove x axis (time)\n",
        "        tot_rows += len(fusion)\n",
        "        clean_data(fusion)\n",
        "        # decimation (optional)\n",
        "        if decimation_factor:\n",
        "          fusion2 = np.empty((fusion.shape[0] // decimation_factor, fusion.shape[1]))\n",
        "          for col in range(0, 4):\n",
        "            #tmp = scipy.signal.decimate(fusion[:, col], decimation_factor)\n",
        "            tmp = fusion[:, col][::decimation_factor]  # simpler method\n",
        "            fusion2[:, col] = tmp[:len(fusion2)]\n",
        "          fusion = fusion2\n",
        "        # windowing\n",
        "        # compute number of windows (lazy way)\n",
        "        i = 0\n",
        "        num_w = 0\n",
        "        while i + window  <= len(fusion):\n",
        "          i += (window - overlap)\n",
        "          num_w += 1\n",
        "        # compute actual windows\n",
        "        x_data_part = np.empty((num_w, window, 4))  # preallocate\n",
        "        i = 0\n",
        "        for w in range(0, num_w):\n",
        "          x_data_part[w] = fusion[i:i + window]\n",
        "          i += (window - overlap)\n",
        "        x_data = np.vstack((x_data, x_data_part))\n",
        "        y_data = np.vstack((y_data, np.full((num_w, 1), category)))\n",
        "        subj_inputs[-1] += num_w\n",
        "\n",
        "  # normalize single windows\n",
        "  for w in x_data:\n",
        "    # remove mean value from ACC\n",
        "    w[:, 0] -= np.mean(w[:, 0])  # acc 1\n",
        "    w[:, 1] -= np.mean(w[:, 1])  # acc 2\n",
        "    w[:, 2] -= np.mean(w[:, 2])  # acc 3\n",
        "    # standardize PPG\n",
        "    w[:, 3] = StandardScaler().fit_transform(w[:, 3].reshape(-1, 1)).reshape((w.shape[0],))  # PPG\n",
        "\n",
        "  print('tot samples:', tot_rows)\n",
        "  print('x_data:', x_data.shape)\n",
        "  print('y_data:', y_data.shape)\n",
        "  print('inputs per subject:', subj_inputs)\n",
        "  if distrib_stat:\n",
        "    n = sum(subj_inputs[:distrib_stat])\n",
        "    print('class distribution (first', distrib_stat, 'subjects):', np.sum(y_data[:n] == 0), np.sum(y_data[:n] == 1), np.sum(y_data[:n] == 2))\n",
        "  return x_data, y_data, subj_inputs\n",
        "\n",
        "\n",
        "def clean_data(fusion):\n",
        "  # some tracks have isolated NaNs\n",
        "  for col in range(0, 4):\n",
        "    ids = np.where(np.isnan(fusion[:, col]))[0]\n",
        "    for row in ids:\n",
        "      fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "  # some PPG tracks have periodic, isolated zeros, resulting in spikes\n",
        "  for col in range(3, 4):\n",
        "    ids = np.where(fusion[:, col] == 0)[0]\n",
        "    for row in ids:\n",
        "      fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "  # many acc. tracks have periodic, single-point spikes of no specific values. Let's test all of them, even if it's slow\n",
        "  for col in range(0, 3):\n",
        "    for row in range(1, len(fusion) - 1):\n",
        "      if abs(fusion[row, col] - fusion[row - 1, col]) > 5000 and abs(fusion[row, col] - fusion[row + 1, col]) > 5000:\n",
        "        fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "\n",
        "\n",
        "def partition_data(subjects):\n",
        "  # subjects = tuple (0-based)\n",
        "  x_part = None\n",
        "  y_part = None\n",
        "  for subj in subjects:\n",
        "    skip = sum(subj_inputs[:subj])\n",
        "    num = subj_inputs[subj]\n",
        "    xx = x_data[skip : skip + num]\n",
        "    yy = y_data[skip : skip + num]\n",
        "    if x_part is None:\n",
        "      x_part = xx.copy()\n",
        "      y_part = yy.copy()\n",
        "    else:\n",
        "      x_part = np.vstack((x_part, xx))  # vstack creates a copy of the data\n",
        "      y_part = np.vstack((y_part, yy))\n",
        "  return x_part, y_part\n",
        "\n",
        "\n",
        "def oversampling(x_data, y_data, subj_inputs, num_subjects):\n",
        "  # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n",
        "  # We want to do that on a per-subject basis, so to keep subjects separate.\n",
        "  # Moreover, we do that only to the first num_subjects subjects, so to leave test subjects unaltered.\n",
        "  x_data_over = None\n",
        "  y_data_over = None\n",
        "  subj_inputs_over = []\n",
        "  skip = 0\n",
        "  for subj_num in subj_inputs[:num_subjects]:\n",
        "    x_part = x_data[skip : skip + subj_num]\n",
        "    y_part = y_data[skip : skip + subj_num]\n",
        "    occurr = (np.sum(y_part == 0), np.sum(y_part == 1), np.sum(y_part == 2))\n",
        "    assert(occurr[0] == max(occurr))\n",
        "    mul = (1, occurr[0] // occurr[1], occurr[0] // occurr[2])\n",
        "    for cl in (1, 2):\n",
        "      mask = y_part[:, 0] == cl\n",
        "      x_dup = x_part[mask].copy()\n",
        "      y_dup = y_part[mask].copy()\n",
        "      for n in range(0, mul[cl] - 1):\n",
        "        x_part = np.vstack((x_part, x_dup))\n",
        "        y_part = np.vstack((y_part, y_dup))\n",
        "    if x_data_over is None:\n",
        "      x_data_over = x_part\n",
        "      y_data_over = y_part\n",
        "    else:\n",
        "      x_data_over = np.vstack((x_data_over, x_part))\n",
        "      y_data_over = np.vstack((y_data_over, y_part))\n",
        "    subj_inputs_over.append(len(x_part))\n",
        "    skip += subj_num\n",
        "  x_data_over = np.vstack((x_data_over, x_data[skip:]))  # subjects not oversampled\n",
        "  y_data_over = np.vstack((y_data_over, y_data[skip:]))\n",
        "  subj_inputs_over.extend(subj_inputs[num_subjects:])\n",
        "  print('After oversampling', num_subjects, 'subjects:')\n",
        "  print('x_data:', x_data_over.shape)\n",
        "  print('y_data:', y_data_over.shape)\n",
        "  print('inputs per subject:', subj_inputs_over)\n",
        "  n = sum(subj_inputs_over[:num_subjects])\n",
        "  print('class distribution (first', num_subjects, 'subjects):', np.sum(y_data_over[:n] == 0), np.sum(y_data_over[:n] == 1), np.sum(y_data_over[:n] == 2))\n",
        "  return x_data_over, y_data_over, subj_inputs_over\n",
        "\n",
        "\n",
        "def create_model(window, dense1, lstm1, lstm2, lstm3 = 0, dropout = 0.2):\n",
        "  print(\"### creating model\")\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.Input(shape = (window, 4)))\n",
        "  if dense1: model.add(Dense(dense1, name = 'dense1'))\n",
        "  model.add(BatchNormalization(name = 'norm'))\n",
        "  #model.add(Dropout(dropout, name = 'drop1'))\n",
        "  model.add(LSTM(lstm1, return_sequences = bool(lstm2), name = 'lstm1'))\n",
        "  if lstm2:\n",
        "    model.add(Dropout(dropout, name = 'drop2'))\n",
        "    model.add(LSTM(lstm2, return_sequences = bool(lstm3), name = 'lstm2'))\n",
        "  if lstm3:\n",
        "    model.add(Dropout(dropout, name = 'drop3'))\n",
        "    model.add(LSTM(lstm3, name = 'lstm3'))\n",
        "  model.add(Dropout(dropout, name = 'drop4'))\n",
        "  model.add(Dense(3, name = 'dense2'))\n",
        "  model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "    optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_session(save_model = False, load_model = None, write_report = True, file_id = ''):\n",
        "\n",
        "  def write_values():\n",
        "    print(time.strftime('%H:%M:%S'), file = out_f)\n",
        "    print('window', window, 'overlap', overlap, 'decimation', decimation, file = out_f)\n",
        "    print('layers', dense1, lstm1, lstm2, lstm3, file = out_f)\n",
        "    print('oversample', oversample, file = out_f)\n",
        "    print('subj_train', permutation, file = out_f)\n",
        "    print('epochs', epochs, file = out_f)\n",
        "    if history is not None:\n",
        "      print('fit_accuracy', [round(x, 4) for x in history.history['accuracy']], file = out_f)\n",
        "    if history is not None and 'val_accuracy' in history.history:\n",
        "      print('fit_val_accuracy', [round(x, 4) for x in history.history['val_accuracy']], file = out_f)\n",
        "    print('subj_test', subjs_test, file = out_f)\n",
        "    print('test_accuracy', round(eval_metrics[1], 4), file = out_f)\n",
        "    print('permutation', perm_index + 1, file = out_f)\n",
        "    print('iteration', iter + 1, file = out_f)\n",
        "    print('time_train', time_train, file = out_f)\n",
        "    print('time_test', time_test, file = out_f)\n",
        "    print(file = out_f)\n",
        "    out_f.flush()\n",
        "\n",
        "  if write_report:\n",
        "    output_file = time.strftime('%Y%m%d-%H%M%S') + '.txt'\n",
        "    out_f = open(working_dir + '/' + output_file, 'w')\n",
        "  # tensorboard stuff\n",
        "  #%rm -rf \"$log_dir_base\"\n",
        "  log_dir = log_dir_base + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "  for perm_index, permutation in enumerate(subjs_train_perm):\n",
        "    assert(type(permutation) == tuple)\n",
        "    assert(len(permutation) == 2)\n",
        "    assert(type(permutation[0]) == tuple)\n",
        "    assert(type(permutation[1]) == tuple)\n",
        "    assert(type(subjs_test) == tuple)\n",
        "    for iter in range(0, iterations):\n",
        "      print('### Permutation', perm_index + 1, 'Iteration', iter + 1)\n",
        "      if load_model is None:\n",
        "        model = create_model(window, dense1, lstm1, lstm2, lstm3)\n",
        "        # model training\n",
        "        x_data_train, y_data_train = partition_data(permutation[0])  # train subjects\n",
        "        x_data_val, y_data_val = partition_data(permutation[1])  # validation subjects, can be None\n",
        "        print(f'### training with {len(x_data_train)} inputs, {len(x_data_val) if x_data_val is not None else 0} validation')\n",
        "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir + f'_{perm_index + 1}_{iter + 1}', histogram_freq = 1)\n",
        "        # train\n",
        "        start_time = time.monotonic()\n",
        "        history = model.fit(x_data_train, y_data_train, epochs = epochs,\n",
        "          validation_data = (x_data_val, y_data_val) if x_data_val is not None else None,\n",
        "          callbacks = [tensorboard_callback])\n",
        "        time_train = time.monotonic() - start_time\n",
        "      else:\n",
        "        # model must match with dataset parameters\n",
        "        model = keras.models.load_model(load_model)\n",
        "        history = None\n",
        "        time_train = 0\n",
        "\n",
        "      # model test\n",
        "      x_data_test, y_data_test = partition_data(subjs_test)\n",
        "      print(f'### testing with {len(x_data_test)} inputs')\n",
        "      start_time = time.monotonic()\n",
        "      eval_metrics = model.evaluate(x_data_test, y_data_test)\n",
        "      time_test = time.monotonic() - start_time\n",
        "      if write_report:\n",
        "        write_values()\n",
        "      if save_model:\n",
        "        # save in both directory and h5 formats (we had problems with both of them sometimes)\n",
        "        model_file_name = f'{working_dir}/{file_id}model_w{window:04d}_o{overlap:03d}_d{decimation:03d}_e{epochs}_t{round(eval_metrics[1] * 10000)}'\n",
        "        #model.save(model_file_name)\n",
        "        model.save(model_file_name + '.h5')\n",
        "  if write_report:\n",
        "    out_f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix3vuTaQVKkZ"
      },
      "source": [
        "---\n",
        "**Start of actual program blocks:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZI32WQnlGFu"
      },
      "source": [
        "# create dataset with given window and overlap\n",
        "if __name__ == '__main__':\n",
        "  window = 1200\n",
        "  overlap = window // 2\n",
        "  oversample = True\n",
        "  decimation = 0\n",
        "  if decimation:\n",
        "    window //= decimation\n",
        "    overlap //= decimation\n",
        "  x_data, y_data, subj_inputs = create_dataset(window, overlap, decimation, distrib_stat = 5)\n",
        "  if oversample:\n",
        "    x_data, y_data, subj_inputs = oversampling(x_data, y_data, subj_inputs, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-W5HYh5b41G"
      },
      "source": [
        "# create and train model\n",
        "if __name__ == '__main__':\n",
        "  dense1 = 32\n",
        "  lstm1 = 32\n",
        "  lstm2 = 32\n",
        "  lstm3 = 32\n",
        "  subjs_train_perm = ( ((0, 1, 2, 3, 4), ()), )  # final model\n",
        "  #subjs_train_perm = ( ((0, 1, 2, 3), (4,)), ((0, 1, 2, 4), (3,)), ((0, 1, 3, 4), (2,)), ((0, 2, 3, 4), (1,)), ((1, 2, 3, 4), (0,)) )  # cross-validation\n",
        "  subjs_test = (5, 6)\n",
        "  epochs = 100\n",
        "  iterations = 1  # repeat every test to cope with randomness\n",
        "  train_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBj48t7ocH4q"
      },
      "source": [
        "# model test only\n",
        "if __name__ == '__main__':\n",
        "  x_data_test, y_data_test = partition_data(subjs_test)\n",
        "  print(f'### testing with {len(x_data_test)} inputs')\n",
        "  eval_metrics = model.evaluate(x_data_test, y_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNYFqG128gjV"
      },
      "source": [
        "# display tensorboard\n",
        "if __name__ == '__main__':\n",
        "  %tensorboard --logdir \"$log_dir_base\"\n",
        "  pass  # when exporting as .py, previous line is commented out and an empty block generates an error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBANWGTO-8DY"
      },
      "source": [
        "# create confusion matrix\n",
        "if __name__ == '__main__':\n",
        "  import pandas as pd\n",
        "  import seaborn\n",
        "  y_pred = model.predict_classes(x_data_test)\n",
        "  con_mat = tf.math.confusion_matrix(labels = y_data_test, predictions = y_pred).numpy()\n",
        "  con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis = 1)[:, np.newaxis], decimals = 2)\n",
        "  classes = ['resting', 'squat', 'stepper']\n",
        "  con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n",
        "  #con_mat_df['squat']['stepper']=0.11  # fix sum not 1 due to rounding\n",
        "  #print(con_mat_df)\n",
        "  figure = plt.figure(figsize = (5, 5))\n",
        "  seaborn.heatmap(con_mat_df, annot = True, cmap = plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(working_dir + '/confusion_matrix.eps', format='eps')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrZGGTOWlC71"
      },
      "source": [
        "# convert Keras model to tflite\n",
        "if __name__ == '__main__':\n",
        "  m = keras.models.load_model(working_dir + '/model_1200_600_100_9428.h5')\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(m)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]  # dynamic range quantization, reduces size\n",
        "  tflite_model = converter.convert()\n",
        "  with open(working_dir + '/model_1200_600_100_9443.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}